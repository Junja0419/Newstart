# 현재 로직:

### 전체 로직 흐름

1. main.py에서 메인 함수 실행

-   크롤링을 시작하고 섹션별로 데이터를 처리하는 메인 로직을 담당.

2. crawler 모듈로 섹션별 크롤링 실행

-   각 섹션의 URL에 따라 크롤링을 수행.
-   뉴스 리스트를 가져오고 상세 크롤링을 통해 content와 first_image를 포함한 모든 데이터를 수집.
-   Selenium 비동기 처리를 통해 상세 크롤링을 병렬로 실행.

3. 섹션별 JSON 파일 생성

-   크롤링이 완료된 데이터를 JSON 형식으로 저장.
-   각 섹션별로 하나의 JSON 파일이 생성됨 (총 6개 파일).

4. 6개 섹션에 대해 반복

-   정치, 경제, 사회, 생활/문화, 세계, IT/과학 섹션을 순차적으로 처리.
-   각 섹션별 크롤링 및 JSON 파일 생성.

5. AI를 통해 모든 JSON 파일의 content 업데이트

-   ai.py 모듈을 사용하여 각 JSON 파일의 content를 업데이트.
-   AI 처리가 완료된 데이터를 다시 JSON 형식으로 저장.

6. REST API 호출

-   최종 업데이트된 데이터를 REST API를 통해 백엔드로 전송.

# 주요 모듈의 역할

### main.py

-   섹션별로 크롤링을 실행하고 결과를 관리.
-   JSON 파일 생성 및 AI 처리, REST API 호출의 흐름을 제어.

### crawler.py

-   섹션별 뉴스 데이터를 크롤링 (리스트 및 상세 데이터 포함).
-   비동기 처리로 Selenium 성능 최적화.

### ai.py

-   JSON 데이터를 받아 content 필드를 AI로 업데이트.
-   AI 모델 호출 로직 포함.

### api.py

-   최종 데이터를 백엔드 API로 전송.

# Selenium 리소스 관리:

-   각 스레드에서 독립적인 Selenium 세션을 생성하므로 max_workers 값을 적절히 조정해야 함.

# 에러 핸들링:

-   크롤링 실패, AI 처리 실패, API 호출 실패 시 오류를 기록하고 다음 작업으로 넘어가도록 개선할 필요 있음.

# REST API 구축

1. 주요 엔드포인트

-   /crawl (POST):
    -   특정 섹션을 크롤링하고 AI로 content를 업데이트.
    -   데이터를 JSON 파일로 저장.
-   /data/<section> (GET):
    -   특정 섹션의 데이터를 반환.
    -   저장된 JSON 파일에서 데이ㅓ를 로드하여 반환.
-   /data/all (GET):
    -   모든 섹션의 데이터를 통합하여 반환.
-   / (GET):
    -   API 상태 확인용 기본 엔드포인트.

2. 데이터 저장

-   데이터를 JSON 파일로 저장하여 API 호출 간 데이터를 캐시.
-   저장 경로는 ./data 디렉터리로 설정.

3. 비동기 크롤링

-   크롤러는 asyncio.run 으로 비동기 호출.
